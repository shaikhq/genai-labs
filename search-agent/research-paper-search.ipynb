{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dabdf42",
   "metadata": {},
   "source": [
    "![Agent Workflow](flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06736757",
   "metadata": {},
   "source": [
    "## AI Research Assistant using Watsonx.ai, LangGraph, and ReAct\n",
    "\n",
    "This notebook showcases a personal AI research assistant built using the **IBM Watsonx.ai platform**, **LangGraph framework**, and the **ReAct agent architecture**. The assistant automates the end-to-end process of discovering and summarizing recent research papers from top scientific conferences.\n",
    "\n",
    "### üîç Key Features and Workflow:\n",
    "\n",
    "* Accepts a user prompt describing a research topic.\n",
    "* Uses **Watsonx.ai LLM** to identify the relevant research domain (e.g., NLP, ML).\n",
    "* Searches online repositories like **arXiv** for recent, relevant papers.\n",
    "* Returns the top three papers with abstracts.\n",
    "* On request, summarizes the papers using **LLM summarization tools**.\n",
    "* Built with **LangGraph**, enabling multi-step planning and tool usage via the **ReAct pattern**.\n",
    "* Hosted and orchestrated via **Watsonx.ai** services.\n",
    "\n",
    "This AI-powered workflow streamlines literature review and research exploration, making it easier to stay up-to-date with the latest scientific advancements.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e3295",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34202855",
   "metadata": {},
   "source": [
    "### Step 1: Load Configuration and Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51bd1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import ChatWatsonx\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from langchain.agents import Tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fba3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads .env file from exactly os.getcwd() + \"/.env\" ‚Äî no parent directory search.\n",
    "load_dotenv(os.getcwd()+\"/.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79ce237",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatWatsonx(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    url=os.getenv(\"WATSONX_URL\"),\n",
    "    apikey=os.getenv(\"WATSONX_API_KEY\"),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        \"decoding_method\": \"greedy\",\n",
    "        \"temperature\": 0,\n",
    "        \"min_new_tokens\": 5,\n",
    "        \"max_new_tokens\": 2000\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4881ab4",
   "metadata": {},
   "source": [
    "### Step 2: Define Tool to Search arXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4397a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e9031",
   "metadata": {},
   "source": [
    "### Step 3: LangChain Tool for Searching Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6958bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "# --- Tool: Search arXiv by query + category ---\n",
    "def search_arxiv(query: str, category: str = \"all\", max_results=3):\n",
    "    import requests\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    url = (\n",
    "        f\"http://export.arxiv.org/api/query?\"\n",
    "        f\"search_query=cat:{category}+AND+all:{query}\"\n",
    "        f\"&start=0&max_results={max_results}&sortBy=lastUpdatedDate\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    root = ET.fromstring(response.text)\n",
    "    ns = {\"atom\": \"http://www.w3.org/2005/Atom\"}\n",
    "    papers = []\n",
    "\n",
    "    for entry in root.findall(\"atom:entry\", ns):\n",
    "        title = entry.find(\"atom:title\", ns).text.strip().replace(\"\\n\", \" \")\n",
    "        abstract = entry.find(\"atom:summary\", ns).text.strip().replace(\"\\n\", \" \")\n",
    "        published = entry.find(\"atom:published\", ns).text.strip()\n",
    "        year = published[:4]\n",
    "\n",
    "        # Look for the PDF link\n",
    "        pdf_link = None\n",
    "        for link in entry.findall(\"atom:link\", ns):\n",
    "            if link.attrib.get(\"type\") == \"application/pdf\":\n",
    "                pdf_link = link.attrib.get(\"href\")\n",
    "                break\n",
    "\n",
    "        # Fallback to arXiv abstract page if PDF not found\n",
    "        if not pdf_link:\n",
    "            pdf_link = entry.find(\"atom:id\", ns).text.strip()\n",
    "\n",
    "        papers.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"year\": year,\n",
    "            \"url\": pdf_link\n",
    "        })\n",
    "\n",
    "    return papers\n",
    "\n",
    "def identify_domain_function(user_query: str) -> str:\n",
    "    prompt = (\n",
    "        \"Classify the academic research domain of the following query. \"\n",
    "        \"Choose one of the following: 'cs.DB' (databases), 'cs.LG' (machine learning), \"\n",
    "        \"'cs.AI' (artificial intelligence), 'cs.CL' (natural language processing), \"\n",
    "        \"'cs.IR' (information retrieval).\\n\\n\"\n",
    "        f\"Query: {user_query}\\n\\nReturn only the domain code (e.g., cs.DB).\"\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "\n",
    "def paper_tool_function(query: str, category: str):\n",
    "    results = search_arxiv(query, category)\n",
    "    formatted = []\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        formatted.append(\n",
    "            f\"{i}. **Title**: {r['title']}\\n\"\n",
    "            f\"   - **Abstract**: {r['abstract']}\\n\"\n",
    "            f\"   - **Year**: {r['year']}\\n\"\n",
    "            f\"   - **PDF**: [{r['title']}]({r['url']})\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# --- Tool: Summarizer ---\n",
    "def summarize_text_function(papers: list[dict]):\n",
    "    summaries = []\n",
    "    for paper in papers:\n",
    "        prompt = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Abstract: {paper['abstract']}\\n\"\n",
    "            f\"Summarize the key points in 2-3 bullets.\"\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "        summaries.append(\n",
    "            f\"**{paper['title']}**\\n\"\n",
    "            f\"{response.content}\\n\"\n",
    "            f\"üîó [PDF Link]({paper['url']})\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2e2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# --- Input schema for search_papers tool ---\n",
    "class SearchPapersInput(BaseModel):\n",
    "    query: str\n",
    "    category: str\n",
    "\n",
    "# --- Tools setup ---\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"identify_domain\",\n",
    "        func=identify_domain_function,\n",
    "        description=(\n",
    "            \"Use this to identify the arXiv domain (e.g., cs.DB, cs.CL, cs.LG, cs.IR) \"\n",
    "            \"from the user's question or topic.\"\n",
    "        )\n",
    "    ),\n",
    "    StructuredTool(\n",
    "        name=\"search_papers\",\n",
    "        func=paper_tool_function,\n",
    "        description=(\n",
    "            \"Use this to search arXiv for real scientific papers. \"\n",
    "            \"Requires both 'query' and 'category' (like cs.DB).\"\n",
    "        ),\n",
    "        args_schema=SearchPapersInput  # defined earlier with query and category\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"summarize_abstracts\",\n",
    "        func=summarize_text_function,\n",
    "        description=\"Use this to summarize long abstracts or paper lists in 3‚Äì5 bullet points.\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7e112",
   "metadata": {},
   "source": [
    "### 4. Define the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74961665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"\n",
    "You are a research assistant that finds and summarizes real scientific papers using tools.\n",
    "\n",
    "You MUST follow this procedure:\n",
    "\n",
    "1. ALWAYS call `identify_domain` first to determine the arXiv category.\n",
    "2. Then call `search_papers` with the topic and category.\n",
    "3. Wait for the real tool response before doing anything else.\n",
    "4. NEVER fabricate tool responses.\n",
    "5. If the user asks for a summary, call `summarize_abstracts` using the actual abstracts from the `search_papers` result.\n",
    "6. Do NOT include paper titles, authors, or abstracts unless they came directly from the tool result.\n",
    "\n",
    "If the tool has not yet returned results, do not proceed.\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c6459",
   "metadata": {},
   "source": [
    "### Step 4: Create ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a95392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35cfe3e",
   "metadata": {},
   "source": [
    "### Step 5: Final Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc4aa296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "conversation_messages = []  # global or per-session message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c9737db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "conversation_messages = []\n",
    "\n",
    "def run_agent(user_prompt: str):\n",
    "    global conversation_messages\n",
    "\n",
    "    conversation_messages.append(HumanMessage(content=user_prompt))\n",
    "\n",
    "    state = {\n",
    "    \"messages\": conversation_messages,\n",
    "    \"is_last_step\": True,  # ‚úÖ Stop after tool call\n",
    "    \"remaining_steps\": 5\n",
    "    }   \n",
    "\n",
    "    print(\"\\n=== Full Intermediate Execution Trace (Readable) ===\")\n",
    "\n",
    "    stream = agent_executor.stream(state, config={\"verbose\": True})\n",
    "\n",
    "    for step_num, step_state in enumerate(stream):\n",
    "        print(f\"\\n--- Step {step_num + 1} ---\")\n",
    "\n",
    "        # Extract messages from agent or tools or directly\n",
    "        if \"messages\" in step_state:\n",
    "            messages = step_state[\"messages\"]\n",
    "        elif \"agent\" in step_state and \"messages\" in step_state[\"agent\"]:\n",
    "            messages = step_state[\"agent\"][\"messages\"]\n",
    "        elif \"tools\" in step_state and \"messages\" in step_state[\"tools\"]:\n",
    "            messages = step_state[\"tools\"][\"messages\"]\n",
    "        else:\n",
    "            messages = []\n",
    "\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                print(f\"\\nüë§ User:\\n{msg.content}\")\n",
    "\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                content = msg.content.strip() or \"[No assistant content]\"\n",
    "                print(f\"\\nü§ñ Assistant:\\n{content}\")\n",
    "\n",
    "                tool_calls = msg.additional_kwargs.get(\"tool_calls\", [])\n",
    "                for call in tool_calls:\n",
    "                    fn_name = call[\"function\"][\"name\"]\n",
    "                    args = call[\"function\"][\"arguments\"]\n",
    "                    print(f\"üß† Tool Call Planned ‚Üí {fn_name} with args: {args}\")\n",
    "\n",
    "                if \"token_usage\" in msg.response_metadata:\n",
    "                    usage = msg.response_metadata[\"token_usage\"]\n",
    "                    print(f\"üìä Token usage: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}\")\n",
    "\n",
    "                conversation_messages.append(msg)\n",
    "\n",
    "            elif isinstance(msg, ToolMessage):\n",
    "                print(f\"\\nüîß Tool Output ({msg.name}):\\n{msg.content[:800]}\")\n",
    "                if len(msg.content) > 800:\n",
    "                    print(\"... [truncated]\")\n",
    "                conversation_messages.append(msg)\n",
    "\n",
    "        # Debug other non-message state keys if needed\n",
    "        other_keys = set(step_state.keys()) - {\"messages\", \"agent\", \"tools\"}\n",
    "        for key in other_keys:\n",
    "            print(f\"\\nüîç State[{key}]: {step_state[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3be5bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_conversation():\n",
    "    global conversation_messages\n",
    "    conversation_messages = []\n",
    "    print(\"üîÑ Conversation reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3efe3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Conversation reset.\n",
      "\n",
      "=== Full Intermediate Execution Trace (Readable) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1 ---\n",
      "\n",
      "ü§ñ Assistant:\n",
      "[No assistant content]\n",
      "üß† Tool Call Planned ‚Üí search_papers with args: {\"query\": \"database query plan representation transformers\", \"category\": \"cs.DB\"}\n",
      "üìä Token usage: prompt=555, completion=36, total=591\n",
      "\n",
      "--- Step 2 ---\n",
      "\n",
      "üîß Tool Output (search_papers):\n",
      "1. **Title**: SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale   SQL Generation\n",
      "   - **Abstract**: High quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent \n",
      "... [truncated]\n",
      "\n",
      "--- Step 3 ---\n",
      "\n",
      "ü§ñ Assistant:\n",
      "Here are three recent papers on database query plan representation using transformers since 2024:\n",
      "\n",
      "1. **SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation** (2025)\n",
      "   - This paper proposes a multi-agent framework, SQL-Factory, for high-quality and large-scale SQL generation. It decomposes the generation process into three collaborative teams: Generation Team, Expansion Team, and Management Team. The framework ensures a balanced trade-off between diversity, scalability, and generation cost.\n",
      "\n",
      "2. **S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)** (2025)\n",
      "   - This paper introduces the problem of subgraph similarity search under aggregated neighbor difference semantics (S3AND). It proposes two effective pruning methods, keyword set and aggregated neighbor difference lower bound pruning, to reduce the S3AND search space. The paper also presents an effective indexing mechanism for efficient S3AND query answering.\n",
      "\n",
      "3. **A Unified Approach for Multi-Granularity Search over Spatial Datasets** (2024)\n",
      "   - This paper presents Spadas, a multi-granularity spatial data search system that supports both dataset and data point search operations. It proposes a unified index to organize data reasonably and remove outliers in datasets, improving query efficiency. The paper also introduces pruning mechanisms to filter out non-relevant datasets and points.\n",
      "üìä Token usage: prompt=1700, completion=321, total=2021\n"
     ]
    }
   ],
   "source": [
    "reset_conversation()\n",
    "run_agent(\"Find recent 3 papers on database query plan representation using transformers since 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "547e8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Intermediate Execution Trace (Readable) ===\n",
      "\n",
      "--- Step 1 ---\n",
      "\n",
      "ü§ñ Assistant:\n",
      "Here's a summary of the three papers:\n",
      "\n",
      "1. **SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation** (2025)\n",
      "   - The paper proposes a multi-agent framework, SQL-Factory, for high-quality and large-scale SQL generation. It decomposes the generation process into three collaborative teams: Generation Team, Expansion Team, and Management Team. The framework ensures a balanced trade-off between diversity, scalability, and generation cost. SQL-Factory generates over 300,000 SQL queries with less than $200 API cost, achieving higher diversity compared to other methods and significantly improving model performance in various downstream tasks.\n",
      "\n",
      "2. **S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)** (2025)\n",
      "   - This paper introduces the problem of subgraph similarity search under aggregated neighbor difference semantics (S3AND). It proposes two effective pruning methods, keyword set and aggregated neighbor difference lower bound pruning, to reduce the S3AND search space. The paper also presents an effective indexing mechanism for efficient S3AND query answering. Extensive experiments demonstrate the effectiveness of the S3AND approach over both real and synthetic graphs under various parameter settings.\n",
      "\n",
      "3. **A Unified Approach for Multi-Granularity Search over Spatial Datasets** (2024)\n",
      "   - The paper presents Spadas, a multi-granularity spatial data search system that supports both dataset and data point search operations. It proposes a unified index to organize data reasonably and remove outliers in datasets, improving query efficiency. The paper also introduces pruning mechanisms to filter out non-relevant datasets and points. An online spatial data search system of Spadas is implemented and made accessible to users. The experimental evaluation using six spatial data repositories shows that Spadas achieves orders of magnitude faster query efficiency than state-of-the-art algorithms.\n",
      "üìä Token usage: prompt=2482, completion=427, total=2909\n"
     ]
    }
   ],
   "source": [
    "run_agent(\"Can you summarize the papers from search?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wxai-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
